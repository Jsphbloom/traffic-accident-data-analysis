{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Short project description. Your bottom line up front (BLUF) insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is relevant in the fact that there are many accidents each day, causing damage to the wellbeing of people and property. Using this model, we can gain insight into correlative factors for accidents to occur. Efforts can go towards building/repairing infrastructure, modifying security and adjusting insurance policies with these insights in mind.\n",
    "\n",
    "How relevant is the day of the week when it comes to the severity of crashes?\n",
    "\n",
    "What potentially unseen factors play a larger role in causing accidents than one may assume?\n",
    "\n",
    "How can one use the statistical knowledge gained here in practical application to help prevent accidents in the future?\n",
    "\n",
    "These are important questions that can be answered through careful analysis of the data provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant imports here\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/Joe/Downloads/archive/US_Accidents_March23.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()\n",
    "df.shape\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_to_impute = ['Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n",
    "categorical_cols_to_impute = ['Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "\n",
    "for col in numerical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with median value ({median_val:.2f}).\")\n",
    "\n",
    "for col in categorical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with mode value ('{mode_val}').\")\n",
    "\n",
    "# Dropping columns with a very high percentage of missing values that are not critical for our analysis.\n",
    "# Example: 'Number' often has many missing values.\n",
    "if 'Number' in df.columns:\n",
    "    df.drop('Number', axis=1, inplace=True)\n",
    "    print(\"Dropped 'Number' column due to high number of missing values.\")\n",
    "\n",
    "# Step 4.2: Handling Outliers\n",
    "print(\"\\nStep 4.2: Handling Outliers...\")\n",
    "# We'll use the Interquartile Range (IQR) method to identify and cap outliers\n",
    "# for the 'Temperature(F)' column as an example.\n",
    "if 'Temperature(F)' in df.columns:\n",
    "    Q1 = df['Temperature(F)'].quantile(0.25)\n",
    "    Q3 = df['Temperature(F)'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Capping the outliers\n",
    "    original_outliers = df[(df['Temperature(F)'] < lower_bound) | (df['Temperature(F)'] > upper_bound)].shape[0]\n",
    "    df['Temperature(F)'] = np.where(df['Temperature(F)'] > upper_bound, upper_bound, df['Temperature(F)'])\n",
    "    df['Temperature(F)'] = np.where(df['Temperature(F)'] < lower_bound, lower_bound, df['Temperature(F)'])\n",
    "    print(f\"Capped {original_outliers} outliers in 'Temperature(F)' using IQR method.\")\n",
    "    print(f\"Temperature(F) values are now capped between {lower_bound:.2f} and {upper_bound:.2f}.\")\n",
    "\n",
    "# Step 4.3: Converting Data Types\n",
    "print(\"\\nStep 4.3: Converting Data Types...\")\n",
    "# Convert time-related columns from object to datetime\n",
    "for col in ['Start_Time', 'End_Time']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"Converted '{col}' to datetime objects.\")\n",
    "\n",
    "# Drop rows where conversion might have failed (resulted in NaT)\n",
    "df.dropna(subset=['Start_Time', 'End_Time'], inplace=True)\n",
    "\n",
    "# Step 4.4: Feature Engineering\n",
    "print(\"\\nStep 4.4: Feature Engineering...\")\n",
    "# Create derived features from the 'Start_Time' column\n",
    "if 'Start_Time' in df.columns:\n",
    "    df['Hour'] = df['Start_Time'].dt.hour\n",
    "    df['DayOfWeek'] = df['Start_Time'].dt.dayofweek # Monday=0, Sunday=6\n",
    "    df['Month'] = df['Start_Time'].dt.month\n",
    "    print(\"Created 'Hour', 'DayOfWeek', and 'Month' features from 'Start_Time'.\")\n",
    "\n",
    "# Calculate the duration of the accident in minutes\n",
    "if 'Start_Time' in df.columns and 'End_Time' in df.columns:\n",
    "    df['Duration(min)'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60\n",
    "    print(\"Created 'Duration(min)' feature.\")\n",
    "\n",
    "\n",
    "# --- 5. Final Data Inspection ---\n",
    "print(\"\\n--- Final Data Inspection ---\")\n",
    "print(\"Dataset Information after cleaning and preprocessing:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nChecking for any remaining missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# --- 6. Descriptive Statistics (Post-Processing) ---\n",
    "print(\"\\n--- Descriptive Statistics (Post-Processing) ---\")\n",
    "print(\"Summary statistics for numerical columns after cleaning:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the final preprocessed dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_to_impute = ['Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n",
    "categorical_cols_to_impute = ['Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "\n",
    "for col in numerical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with median value ({median_val:.2f}).\")\n",
    "\n",
    "for col in categorical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with mode value ('{mode_val}').\")\n",
    "\n",
    "# Dropping columns with a very high percentage of missing values that are not critical for our analysis.\n",
    "# Example: 'Number' often has many missing values.\n",
    "if 'Number' in df.columns:\n",
    "    df.drop('Number', axis=1, inplace=True)\n",
    "    print(\"Dropped 'Number' column due to high number of missing values.\")\n",
    "\n",
    "# Step 4.2: Handling Outliers\n",
    "print(\"\\nStep 4.2: Handling Outliers...\")\n",
    "# We'll use the Interquartile Range (IQR) method to identify and cap outliers\n",
    "# for the 'Temperature(F)' column as an example.\n",
    "if 'Temperature(F)' in df.columns:\n",
    "    Q1 = df['Temperature(F)'].quantile(0.25)\n",
    "    Q3 = df['Temperature(F)'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Capping the outliers\n",
    "    original_outliers = df[(df['Temperature(F)'] < lower_bound) | (df['Temperature(F)'] > upper_bound)].shape[0]\n",
    "    df['Temperature(F)'] = np.where(df['Temperature(F)'] > upper_bound, upper_bound, df['Temperature(F)'])\n",
    "    df['Temperature(F)'] = np.where(df['Temperature(F)'] < lower_bound, lower_bound, df['Temperature(F)'])\n",
    "    print(f\"Capped {original_outliers} outliers in 'Temperature(F)' using IQR method.\")\n",
    "    print(f\"Temperature(F) values are now capped between {lower_bound:.2f} and {upper_bound:.2f}.\")\n",
    "\n",
    "# Step 4.3: Converting Data Types\n",
    "print(\"\\nStep 4.3: Converting Data Types...\")\n",
    "# Convert time-related columns from object to datetime\n",
    "for col in ['Start_Time', 'End_Time']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"Converted '{col}' to datetime objects.\")\n",
    "\n",
    "# Drop rows where conversion might have failed (resulted in NaT)\n",
    "df.dropna(subset=['Start_Time', 'End_Time'], inplace=True)\n",
    "\n",
    "# Step 4.4: Feature Engineering\n",
    "print(\"\\nStep 4.4: Feature Engineering...\")\n",
    "# Create derived features from the 'Start_Time' column\n",
    "if 'Start_Time' in df.columns:\n",
    "    df['Hour'] = df['Start_Time'].dt.hour\n",
    "    df['DayOfWeek'] = df['Start_Time'].dt.dayofweek # Monday=0, Sunday=6\n",
    "    df['Month'] = df['Start_Time'].dt.month\n",
    "    print(\"Created 'Hour', 'DayOfWeek', and 'Month' features from 'Start_Time'.\")\n",
    "\n",
    "# Calculate the duration of the accident in minutes\n",
    "if 'Start_Time' in df.columns and 'End_Time' in df.columns:\n",
    "    df['Duration(min)'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60\n",
    "    print(\"Created 'Duration(min)' feature.\")\n",
    "\n",
    "\n",
    "# --- 5. Final Data Inspection ---\n",
    "print(\"\\n--- Final Data Inspection ---\")\n",
    "print(\"Dataset Information after cleaning and preprocessing:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nChecking for any remaining missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# --- 6. Descriptive Statistics (Post-Processing) ---\n",
    "print(\"\\n--- Descriptive Statistics (Post-Processing) ---\")\n",
    "print(\"Summary statistics for numerical columns after cleaning:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the final preprocessed dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_to_impute = ['Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n",
    "categorical_cols_to_impute = ['Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "\n",
    "for col in numerical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with median value ({median_val:.2f}).\")\n",
    "\n",
    "for col in categorical_cols_to_impute:\n",
    "    if col in df.columns:\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with mode value ('{mode_val}').\")\n",
    "\n",
    "# Dropping columns with a very high percentage of missing values that are not critical for our analysis.\n",
    "# Example: 'Number' often has many missing values.\n",
    "if 'Number' in df.columns:\n",
    "    df.drop('Number', axis=1, inplace=True)\n",
    "    print(\"Dropped 'Number' column due to high number of missing values.\")\n",
    "\n",
    "# Step 4.2: Handling Outliers\n",
    "print(\"\\nStep 4.2: Handling Outliers...\")\n",
    "# We'll use the Interquartile Range (IQR) method to identify and cap outliers\n",
    "# for the 'Temperature(F)' column as an example.\n",
    "if 'Temperature(F)' in df.columns:\n",
    "    Q1 = df['Temperature(F)'].quantile(0.25)\n",
    "    Q3 = df['Temperature(F)'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Capping the outliers\n",
    "    original_outliers = df[(df['Temperature(F)'] < lower_bound) | (df['Temperature(F)'] > upper_bound)].shape[0]\n",
    "    df['Temperature(F)'] = np.where(df['Temperature(F)'] > upper_bound, upper_bound, df['Temperature(F)'])\n",
    "    df['Temperature(F)'] = np.where(df['Temperature(F)'] < lower_bound, lower_bound, df['Temperature(F)'])\n",
    "    print(f\"Capped {original_outliers} outliers in 'Temperature(F)' using IQR method.\")\n",
    "    print(f\"Temperature(F) values are now capped between {lower_bound:.2f} and {upper_bound:.2f}.\")\n",
    "\n",
    "# Step 4.3: Converting Data Types\n",
    "print(\"\\nStep 4.3: Converting Data Types...\")\n",
    "# Convert time-related columns from object to datetime\n",
    "for col in ['Start_Time', 'End_Time']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        print(f\"Converted '{col}' to datetime objects.\")\n",
    "\n",
    "# Drop rows where conversion might have failed (resulted in NaT)\n",
    "df.dropna(subset=['Start_Time', 'End_Time'], inplace=True)\n",
    "\n",
    "# Step 4.4: Feature Engineering\n",
    "print(\"\\nStep 4.4: Feature Engineering...\")\n",
    "# Create derived features from the 'Start_Time' column\n",
    "if 'Start_Time' in df.columns:\n",
    "    df['Hour'] = df['Start_Time'].dt.hour\n",
    "    df['DayOfWeek'] = df['Start_Time'].dt.dayofweek # Monday=0, Sunday=6\n",
    "    df['Month'] = df['Start_Time'].dt.month\n",
    "    print(\"Created 'Hour', 'DayOfWeek', and 'Month' features from 'Start_Time'.\")\n",
    "\n",
    "# Calculate the duration of the accident in minutes\n",
    "if 'Start_Time' in df.columns and 'End_Time' in df.columns:\n",
    "    df['Duration(min)'] = (df['End_Time'] - df['Start_Time']).dt.total_seconds() / 60\n",
    "    print(\"Created 'Duration(min)' feature.\")\n",
    "\n",
    "\n",
    "# --- 5. Final Data Inspection ---\n",
    "print(\"\\n--- Final Data Inspection ---\")\n",
    "print(\"Dataset Information after cleaning and preprocessing:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nChecking for any remaining missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# --- 6. Descriptive Statistics (Post-Processing) ---\n",
    "print(\"\\n--- Descriptive Statistics (Post-Processing) ---\")\n",
    "print(\"Summary statistics for numerical columns after cleaning:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the final preprocessed dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization 3: Accident Hotspots by City ---\n",
    "\n",
    "# Get the top 10 cities with the most accidents\n",
    "top_cities = df['City'].value_counts().nlargest(10)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=top_cities.values, y=top_cities.index, palette=\"plasma\", orient='h')\n",
    "\n",
    "plt.title('Top 10 Cities by Number of Accidents', fontsize=16)\n",
    "plt.xlabel('Number of Accidents')\n",
    "plt.ylabel('City')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "print(\"--- 1. Chi-Square Test: Day of Week vs. Sunrise/Sunset ---\")\n",
    "\n",
    "# H₀ (Null Hypothesis): There is no association between DayOfWeek and Sunrise_Sunset.\n",
    "# H₁ (Alternative Hypothesis): There is an association between DayOfWeek and Sunrise_Sunset.\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['DayOfWeek'], df['Sunrise_Sunset'])\n",
    "print(\"\\nContingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Perform the Chi-Square test\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nChi-Square Statistic: {chi2:.2f}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"\\nConclusion: We reject the null hypothesis (p < 0.05).\")\n",
    "    print(\"There is a statistically significant association between the day of the week and whether an accident occurs during the day or night.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: We fail to reject the null hypothesis (p >= 0.05).\")\n",
    "    print(\"There is no statistically significant association between the day of the week and Sunrise_Sunset.\")\n",
    "\n",
    "# --- 2. ANOVA: Temperature vs. Accident Severity ---\n",
    "# We want to test if the mean temperature is significantly different across various accident severity levels.\n",
    "\n",
    "print(\"\\n\\n--- 2. ANOVA: Temperature vs. Accident Severity ---\")\n",
    "\n",
    "# H₀ (Null Hypothesis): The mean temperature is the same for all severity levels.\n",
    "# H₁ (Alternative Hypothesis): At least one severity level has a different mean temperature.\n",
    "\n",
    "# Prepare data for ANOVA\n",
    "severity_groups = [df['Temperature(F)'][df['Severity'] == i] for i in sorted(df['Severity'].unique())]\n",
    "\n",
    "# Assumption Check 1: Homogeneity of Variances (Levene's Test)\n",
    "levene_stat, levene_p = stats.levene(*severity_groups)\n",
    "print(f\"\\nLevene's Test for Homogeneity of Variances: P-value = {levene_p:.3f}\")\n",
    "if levene_p < alpha:\n",
    "    print(\"Levene's test is significant (p < 0.05), suggesting variances are not equal.\")\n",
    "    use_kruskal = True\n",
    "else:\n",
    "    print(\"Levene's test is not significant (p >= 0.05), variances are assumed to be equal.\")\n",
    "    use_kruskal = False\n",
    "\n",
    "# Assumption Check 2: Normality (Shapiro-Wilk Test)\n",
    "# Note: For very large samples, this test is almost always significant. We proceed with caution.\n",
    "# We'll check one group as an example.\n",
    "shapiro_stat, shapiro_p = stats.shapiro(severity_groups[0].sample(min(5000, len(severity_groups[0]))))\n",
    "print(f\"Shapiro-Wilk Test for Normality (Severity 2 sample): P-value = {shapiro_p:.3f}\")\n",
    "if shapiro_p < alpha:\n",
    "    print(\"Shapiro-Wilk test is significant (p < 0.05), suggesting the data may not be normally distributed.\")\n",
    "    use_kruskal = True\n",
    "else:\n",
    "    print(\"Shapiro-Wilk test is not significant (p >= 0.05), normality is assumed.\")\n",
    "\n",
    "\n",
    "# Perform the appropriate test\n",
    "if use_kruskal:\n",
    "    print(\"\\nAssumptions for ANOVA were not met. Performing Kruskal-Wallis H-test (non-parametric alternative)...\")\n",
    "    kruskal_stat, kruskal_p = stats.kruskal(*severity_groups)\n",
    "    print(f\"Kruskal-Wallis H-test statistic: {kruskal_stat:.2f}\")\n",
    "    print(f\"P-value: {kruskal_p}\")\n",
    "    p_value_anova = kruskal_p\n",
    "else:\n",
    "    print(\"\\nPerforming One-Way ANOVA...\")\n",
    "    f_stat, p_value_anova = stats.f_oneway(*severity_groups)\n",
    "    print(f\"F-statistic: {f_stat:.2f}\")\n",
    "    print(f\"P-value: {p_value_anova}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value_anova < alpha:\n",
    "    print(\"\\nConclusion: We reject the null hypothesis (p < 0.05).\")\n",
    "    print(\"There is a statistically significant difference in mean temperature across different accident severity levels.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: We fail to reject the null hypothesis (p >= 0.05).\")\n",
    "    print(\"There is no statistically significant difference in mean temperature across different accident severity levels.\")\n",
    "\n",
    "\n",
    "# --- 3. Correlation Analysis ---\n",
    "# We will analyze the correlation between key numerical features to understand their relationships.\n",
    "\n",
    "print(\"\\n\\n--- 3. Correlation Analysis ---\")\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numerical_cols = ['Severity', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Duration(min)']\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Visualize the correlation matrix with a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
